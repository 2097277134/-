{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left camera: [ 848x800  p[425.602 399.606]  f[285.845 285.904]  Kannala Brandt4 [-0.00781346 0.0452052 -0.0423382 0.00766618 0] ]\n",
      "Right camera: [ 848x800  p[420.455 401.144]  f[286.246 286.3]  Kannala Brandt4 [-0.00617388 0.0400734 -0.0371676 0.00591487 0] ]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "## License: Apache 2.0. See LICENSE file in root directory.\n",
    "## Copyright(c) 2019 Intel Corporation. All Rights Reserved.\n",
    "# Python 2/3 compatibility\n",
    "from __future__ import print_function\n",
    "\n",
    "\"\"\"\n",
    "This example shows how to use T265 intrinsics and extrinsics in OpenCV to\n",
    "asynchronously compute depth maps from T265 fisheye images on the host.\n",
    "T265 is not a depth camera and the quality of passive-only depth options will\n",
    "always be limited compared to (e.g.) the D4XX series cameras. However, T265 does\n",
    "have two global shutter cameras in a stereo configuration, and in this example\n",
    "we show how to set up OpenCV to undistort the images and compute stereo depth\n",
    "from them.\n",
    "Getting started with python3, OpenCV and T265 on Ubuntu 16.04:\n",
    "First, set up the virtual enviroment:\n",
    "$ apt-get install python3-venv  # install python3 built in venv support\n",
    "$ python3 -m venv py3librs      # create a virtual environment in pylibrs\n",
    "$ source py3librs/bin/activate  # activate the venv, do this from every terminal\n",
    "$ pip install opencv-python     # install opencv 4.1 in the venv\n",
    "$ pip install pyrealsense2      # install librealsense python bindings\n",
    "Then, for every new terminal:\n",
    "$ source py3librs/bin/activate  # Activate the virtual environment\n",
    "$ python3 t265_stereo.py        # Run the example\n",
    "\"\"\"\n",
    "\n",
    "# First import the library\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "# Import OpenCV and numpy\n",
    "import cv2\n",
    "import numpy as np\n",
    "from math import tan, pi\n",
    "\n",
    "\"\"\"\n",
    "In this section, we will set up the functions that will translate the camera\n",
    "intrinsics and extrinsics from librealsense into parameters that can be used\n",
    "with OpenCV.\n",
    "The T265 uses very wide angle lenses, so the distortion is modeled using a four\n",
    "parameter distortion model known as Kanalla-Brandt. OpenCV supports this\n",
    "distortion model in their \"fisheye\" module, more details can be found here:\n",
    "https://docs.opencv.org/3.4/db/d58/group__calib3d__fisheye.html\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Returns R, T transform from src to dst\n",
    "\"\"\"\n",
    "def get_extrinsics(src, dst):\n",
    "    extrinsics = src.get_extrinsics_to(dst)\n",
    "    R = np.reshape(extrinsics.rotation, [3,3]).T\n",
    "    T = np.array(extrinsics.translation)\n",
    "    return (R, T)\n",
    "\n",
    "\"\"\"\n",
    "Returns a camera matrix K from librealsense intrinsics\n",
    "\"\"\"\n",
    "def camera_matrix(intrinsics):\n",
    "    return np.array([[intrinsics.fx,             0, intrinsics.ppx],\n",
    "                     [            0, intrinsics.fy, intrinsics.ppy],\n",
    "                     [            0,             0,              1]])\n",
    "\n",
    "\"\"\"\n",
    "Returns the fisheye distortion from librealsense intrinsics\n",
    "\"\"\"\n",
    "def fisheye_distortion(intrinsics):\n",
    "    return np.array(intrinsics.coeffs[:4])\n",
    "\n",
    "# Set up a mutex to share data between threads \n",
    "from threading import Lock\n",
    "frame_mutex = Lock()\n",
    "frame_data = {\"left\"  : None,\n",
    "              \"right\" : None,\n",
    "              \"timestamp_ms\" : None\n",
    "              }\n",
    "\n",
    "\"\"\"\n",
    "This callback is called on a separate thread, so we must use a mutex\n",
    "to ensure that data is synchronized properly. We should also be\n",
    "careful not to do much work on this thread to avoid data backing up in the\n",
    "callback queue.\n",
    "\"\"\"\n",
    "def callback(frame):\n",
    "    global frame_data\n",
    "    if frame.is_frameset():\n",
    "        frameset = frame.as_frameset()\n",
    "        f1 = frameset.get_fisheye_frame(1).as_video_frame()\n",
    "        f2 = frameset.get_fisheye_frame(2).as_video_frame()\n",
    "        left_data = np.asanyarray(f1.get_data())\n",
    "        right_data = np.asanyarray(f2.get_data())\n",
    "        ts = frameset.get_timestamp()\n",
    "        frame_mutex.acquire()\n",
    "        frame_data[\"left\"] = left_data\n",
    "        frame_data[\"right\"] = right_data\n",
    "        frame_data[\"timestamp_ms\"] = ts\n",
    "        frame_mutex.release()\n",
    "\n",
    "# Declare RealSense pipeline, encapsulating the actual device and sensors\n",
    "pipe = rs.pipeline()\n",
    "\n",
    "# Build config object and stream everything\n",
    "cfg = rs.config()\n",
    "\n",
    "# Start streaming with our callback\n",
    "pipe.start(cfg, callback)\n",
    "\n",
    "try:\n",
    "    # Set up an OpenCV window to visualize the results\n",
    "    WINDOW_TITLE = 'Realsense'\n",
    "    cv2.namedWindow(WINDOW_TITLE, cv2.WINDOW_NORMAL)\n",
    "\n",
    "    # Configure the OpenCV stereo algorithm. See\n",
    "    # https://docs.opencv.org/3.4/d2/d85/classcv_1_1StereoSGBM.html for a\n",
    "    # description of the parameters\n",
    "    window_size = 5\n",
    "    min_disp = 0\n",
    "    # must be divisible by 16\n",
    "    num_disp = 112 - min_disp\n",
    "    max_disp = min_disp + num_disp\n",
    "    stereo = cv2.StereoSGBM_create(minDisparity = min_disp,\n",
    "                                   numDisparities = num_disp,\n",
    "                                   blockSize = 16,\n",
    "                                   P1 = 8*3*window_size**2,\n",
    "                                   P2 = 32*3*window_size**2,\n",
    "                                   disp12MaxDiff = 1,\n",
    "                                   uniquenessRatio = 10,\n",
    "                                   speckleWindowSize = 100,\n",
    "                                   speckleRange = 32)\n",
    "\n",
    "    # Retreive the stream and intrinsic properties for both cameras\n",
    "    profiles = pipe.get_active_profile()\n",
    "    streams = {\"left\"  : profiles.get_stream(rs.stream.fisheye, 1).as_video_stream_profile(),\n",
    "               \"right\" : profiles.get_stream(rs.stream.fisheye, 2).as_video_stream_profile()}\n",
    "    intrinsics = {\"left\"  : streams[\"left\"].get_intrinsics(),\n",
    "                  \"right\" : streams[\"right\"].get_intrinsics()}\n",
    "\n",
    "    # Print information about both cameras\n",
    "    print(\"Left camera:\",  intrinsics[\"left\"])\n",
    "    print(\"Right camera:\", intrinsics[\"right\"])\n",
    "\n",
    "    # Translate the intrinsics from librealsense into OpenCV\n",
    "    K_left  = camera_matrix(intrinsics[\"left\"])\n",
    "    D_left  = fisheye_distortion(intrinsics[\"left\"])\n",
    "    K_right = camera_matrix(intrinsics[\"right\"])\n",
    "    D_right = fisheye_distortion(intrinsics[\"right\"])\n",
    "    (width, height) = (intrinsics[\"left\"].width, intrinsics[\"left\"].height)\n",
    "\n",
    "    # Get the relative extrinsics between the left and right camera\n",
    "    (R, T) = get_extrinsics(streams[\"left\"], streams[\"right\"])\n",
    "\n",
    "    # We need to determine what focal length our undistorted images should have\n",
    "    # in order to set up the camera matrices for initUndistortRectifyMap.  We\n",
    "    # could use stereoRectify, but here we show how to derive these projection\n",
    "    # matrices from the calibration and a desired height and field of view\n",
    "\n",
    "    # We calculate the undistorted focal length:\n",
    "    #\n",
    "    #         h\n",
    "    # -----------------\n",
    "    #  \\      |      /\n",
    "    #    \\    | f  /\n",
    "    #     \\   |   /\n",
    "    #      \\ fov /\n",
    "    #        \\|/\n",
    "    stereo_fov_rad = 90 * (pi/180)  # 90 degree desired fov\n",
    "    stereo_height_px = 300          # 300x300 pixel stereo output\n",
    "    stereo_focal_px = stereo_height_px/2 / tan(stereo_fov_rad/2)\n",
    "\n",
    "    # We set the left rotation to identity and the right rotation\n",
    "    # the rotation between the cameras\n",
    "    R_left = np.eye(3)\n",
    "    R_right = R\n",
    "\n",
    "    # The stereo algorithm needs max_disp extra pixels in order to produce valid\n",
    "    # disparity on the desired output region. This changes the width, but the\n",
    "    # center of projection should be on the center of the cropped image\n",
    "    stereo_width_px = stereo_height_px + max_disp\n",
    "    stereo_size = (stereo_width_px, stereo_height_px)\n",
    "    stereo_cx = (stereo_height_px - 1)/2 + max_disp\n",
    "    stereo_cy = (stereo_height_px - 1)/2\n",
    "\n",
    "    # Construct the left and right projection matrices, the only difference is\n",
    "    # that the right projection matrix should have a shift along the x axis of\n",
    "    # baseline*focal_length\n",
    "    P_left = np.array([[stereo_focal_px, 0, stereo_cx, 0],\n",
    "                       [0, stereo_focal_px, stereo_cy, 0],\n",
    "                       [0,               0,         1, 0]])\n",
    "    P_right = P_left.copy()\n",
    "    P_right[0][3] = T[0]*stereo_focal_px\n",
    "\n",
    "    # Construct Q for use with cv2.reprojectImageTo3D. Subtract max_disp from x\n",
    "    # since we will crop the disparity later\n",
    "    Q = np.array([[1, 0,       0, -(stereo_cx - max_disp)],\n",
    "                  [0, 1,       0, -stereo_cy],\n",
    "                  [0, 0,       0, stereo_focal_px],\n",
    "                  [0, 0, -1/T[0], 0]])\n",
    "\n",
    "    # Create an undistortion map for the left and right camera which applies the\n",
    "    # rectification and undoes the camera distortion. This only has to be done\n",
    "    # once\n",
    "    m1type = cv2.CV_32FC1\n",
    "    (lm1, lm2) = cv2.fisheye.initUndistortRectifyMap(K_left, D_left, R_left, P_left, stereo_size, m1type)\n",
    "    (rm1, rm2) = cv2.fisheye.initUndistortRectifyMap(K_right, D_right, R_right, P_right, stereo_size, m1type)\n",
    "    undistort_rectify = {\"left\"  : (lm1, lm2),\n",
    "                         \"right\" : (rm1, rm2)}\n",
    "\n",
    "    mode = \"stack\"\n",
    "    while True:\n",
    "        # Check if the camera has acquired any frames\n",
    "        frame_mutex.acquire()\n",
    "        valid = frame_data[\"timestamp_ms\"] is not None\n",
    "        frame_mutex.release()\n",
    "\n",
    "        # If frames are ready to process\n",
    "        if valid:\n",
    "            # Hold the mutex only long enough to copy the stereo frames\n",
    "            frame_mutex.acquire()\n",
    "            frame_copy = {\"left\"  : frame_data[\"left\"].copy(),\n",
    "                          \"right\" : frame_data[\"right\"].copy()}\n",
    "            frame_mutex.release()\n",
    "\n",
    "            # Undistort and crop the center of the frames\n",
    "            center_undistorted = {\"left\" : cv2.remap(src = frame_copy[\"left\"],\n",
    "                                          map1 = undistort_rectify[\"left\"][0],\n",
    "                                          map2 = undistort_rectify[\"left\"][1],\n",
    "                                          interpolation = cv2.INTER_LINEAR),\n",
    "                                  \"right\" : cv2.remap(src = frame_copy[\"right\"],\n",
    "                                          map1 = undistort_rectify[\"right\"][0],\n",
    "                                          map2 = undistort_rectify[\"right\"][1],\n",
    "                                          interpolation = cv2.INTER_LINEAR)}\n",
    "\n",
    "            # compute the disparity on the center of the frames and convert it to a pixel disparity (divide by DISP_SCALE=16)\n",
    "            disparity = stereo.compute(center_undistorted[\"left\"], center_undistorted[\"right\"]).astype(np.float32) / 16.0\n",
    "\n",
    "            # re-crop just the valid part of the disparity\n",
    "            disparity = disparity[:,max_disp:]\n",
    "\n",
    "            # convert disparity to 0-255 and color it\n",
    "            disp_vis = 255*(disparity - min_disp)/ num_disp\n",
    "            disp_color = cv2.applyColorMap(cv2.convertScaleAbs(disp_vis,1), cv2.COLORMAP_JET)\n",
    "            color_image = cv2.cvtColor(center_undistorted[\"left\"][:,max_disp:], cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "            if mode == \"stack\":\n",
    "                cv2.imshow(WINDOW_TITLE, np.hstack((color_image, disp_color)))\n",
    "            if mode == \"overlay\":\n",
    "                ind = disparity >= min_disp\n",
    "                color_image[ind, 0] = disp_color[ind, 0]\n",
    "                color_image[ind, 1] = disp_color[ind, 1]\n",
    "                color_image[ind, 2] = disp_color[ind, 2]\n",
    "                cv2.imshow(WINDOW_TITLE, color_image)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('s'): mode = \"stack\"\n",
    "        if key == ord('o'): mode = \"overlay\"\n",
    "        if key == ord('q') or cv2.getWindowProperty(WINDOW_TITLE, cv2.WND_PROP_VISIBLE) < 1:\n",
    "            break\n",
    "    # pipe.stop()\n",
    "    cv2.destroyAllWindows()\n",
    "finally:\n",
    "    pipe.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame #0\n",
      "Position: x: 0, y: 0, z: 0\n",
      "Velocity: x: 0, y: 0, z: 0\n",
      "Acceleration: x: 0, y: 0, z: 0\n",
      "\n",
      "Frame #1\n",
      "Position: x: 0, y: 0, z: 0\n",
      "Velocity: x: 0, y: 0, z: 0\n",
      "Acceleration: x: 0, y: 0, z: 0\n",
      "\n",
      "Frame #2\n",
      "Position: x: 5.57748e-06, y: -0.000180702, z: -1.31622e-05\n",
      "Velocity: x: 0.000387379, y: -0.0125505, z: -0.000914166\n",
      "Acceleration: x: 0.0134525, y: -0.435841, z: -0.0317463\n",
      "\n",
      "Frame #3\n",
      "Position: x: 7.66772e-06, y: -0.000249104, z: -1.78346e-05\n",
      "Velocity: x: 0.000453698, y: -0.0147395, z: -0.00105527\n",
      "Acceleration: x: 0.0134226, y: -0.436066, z: -0.0312202\n",
      "\n",
      "Frame #4\n",
      "Position: x: 1.01067e-05, y: -0.000328334, z: -2.33803e-05\n",
      "Velocity: x: 0.000520881, y: -0.0169218, z: -0.00120498\n",
      "Acceleration: x: 0.0134227, y: -0.436062, z: -0.0310514\n",
      "\n",
      "Frame #5\n",
      "Position: x: 5.75708e-06, y: -0.00042068, z: -1.30963e-05\n",
      "Velocity: x: 9.47877e-05, y: -0.0192587, z: -0.000206361\n",
      "Acceleration: x: -0.0072358, y: -0.442631, z: 0.0171897\n",
      "\n",
      "Frame #6\n",
      "Position: x: 6.29762e-06, y: -0.000521456, z: -1.19706e-05\n",
      "Velocity: x: 6.78757e-05, y: -0.0214054, z: -5.07884e-06\n",
      "Acceleration: x: -0.00691931, y: -0.440291, z: 0.0211079\n",
      "\n",
      "Frame #7\n",
      "Position: x: 6.38791e-06, y: -0.000635211, z: -7.82953e-06\n",
      "Velocity: x: 2.46974e-05, y: -0.0236672, z: 0.000305367\n",
      "Acceleration: x: -0.00716773, y: -0.441982, z: 0.0270574\n",
      "\n",
      "Frame #8\n",
      "Position: x: 5.7094e-06, y: -0.000764409, z: -2.43343e-06\n",
      "Velocity: x: -7.46426e-05, y: -0.0263422, z: 0.000626293\n",
      "Acceleration: x: -0.011208, y: -0.471478, z: 0.0333563\n",
      "\n",
      "Frame #9\n",
      "Position: x: 5.0653e-06, y: -0.000903011, z: 2.52118e-06\n",
      "Velocity: x: -0.000126905, y: -0.0286736, z: 0.000750587\n",
      "Acceleration: x: -0.0105917, y: -0.467135, z: 0.0265913\n",
      "\n",
      "Frame #10\n",
      "Position: x: -1.76182e-05, y: -0.000590555, z: -1.74395e-05\n",
      "Velocity: x: -0.000628071, y: -0.0240292, z: -0.000369364\n",
      "Acceleration: x: -0.0123307, y: -0.458785, z: -0.0145901\n",
      "\n",
      "Frame #11\n",
      "Position: x: -2.03462e-05, y: -0.000712486, z: -2.26549e-05\n",
      "Velocity: x: -0.000623809, y: -0.02585, z: -0.000788907\n",
      "Acceleration: x: -0.00800273, y: -0.427612, z: -0.0378934\n",
      "\n",
      "Frame #12\n",
      "Position: x: -2.35393e-05, y: -0.000847012, z: -2.54198e-05\n",
      "Velocity: x: -0.00066141, y: -0.02797, z: -0.000819187\n",
      "Acceleration: x: -0.00788648, y: -0.426646, z: -0.0302647\n",
      "\n",
      "Frame #13\n",
      "Position: x: -2.69653e-05, y: -0.000992471, z: -2.9582e-05\n",
      "Velocity: x: -0.000702288, y: -0.0301152, z: -0.000946242\n",
      "Acceleration: x: -0.00794135, y: -0.427027, z: -0.0293228\n",
      "\n",
      "Frame #14\n",
      "Position: x: -3.0834e-05, y: -0.00115042, z: -3.12852e-05\n",
      "Velocity: x: -0.000782166, y: -0.0325489, z: -0.000576433\n",
      "Acceleration: x: -0.0113112, y: -0.451887, z: 0.0130876\n",
      "\n",
      "Frame #15\n",
      "Position: x: -3.48852e-05, y: -0.00131891, z: -3.4028e-05\n",
      "Velocity: x: -0.000840616, y: -0.0348233, z: -0.000506687\n",
      "Acceleration: x: -0.0114945, y: -0.453202, z: 0.0135258\n",
      "\n",
      "Frame #16\n",
      "Position: x: -3.92277e-05, y: -0.00149882, z: -3.64157e-05\n",
      "Velocity: x: -0.000894048, y: -0.0370624, z: -0.000448342\n",
      "Acceleration: x: -0.0112225, y: -0.451252, z: 0.0129056\n",
      "\n",
      "Frame #17\n",
      "Position: x: -1.12971e-05, y: -0.00055627, z: 4.13481e-05\n",
      "Velocity: x: -1.84508e-05, y: -0.0245431, z: 0.00183682\n",
      "Acceleration: x: -0.00118336, y: -0.445024, z: 0.0311396\n",
      "\n",
      "Frame #18\n",
      "Position: x: -1.50199e-05, y: -0.000686434, z: 4.78681e-05\n",
      "Velocity: x: -0.000379292, y: -0.0269191, z: 0.00172672\n",
      "Acceleration: x: -0.0217149, y: -0.452454, z: 0.017488\n",
      "\n",
      "Frame #19\n",
      "Position: x: -1.7129e-05, y: -0.000826036, z: 5.82554e-05\n",
      "Velocity: x: -0.000477404, y: -0.0290986, z: 0.00198254\n",
      "Acceleration: x: -0.0211027, y: -0.447882, z: 0.0266514\n",
      "\n",
      "Frame #20\n",
      "Position: x: -1.96089e-05, y: -0.000975617, z: 7.10369e-05\n",
      "Velocity: x: -0.000565236, y: -0.0312023, z: 0.00233368\n",
      "Acceleration: x: -0.020317, y: -0.442065, z: 0.0359662\n",
      "\n",
      "Frame #21\n",
      "Position: x: -2.02052e-05, y: -0.0011314, z: 9.0489e-05\n",
      "Velocity: x: -0.000325715, y: -0.0327224, z: 0.00331112\n",
      "Acceleration: x: 0.00615351, y: -0.39319, z: 0.0888218\n",
      "\n",
      "Frame #22\n",
      "Position: x: -2.17728e-05, y: -0.00130005, z: 0.000108168\n",
      "Velocity: x: -0.000307966, y: -0.0347823, z: 0.00364399\n",
      "Acceleration: x: 0.00497924, y: -0.401697, z: 0.078638\n",
      "\n",
      "Frame #23\n",
      "Position: x: -8.79767e-05, y: -0.000701135, z: 5.12894e-05\n",
      "Velocity: x: -0.00179338, y: -0.0245354, z: 0.00113906\n",
      "Acceleration: x: -0.0166475, y: -0.452045, z: -0.0015086\n",
      "\n",
      "Frame #24\n",
      "Position: x: -9.77158e-05, y: -0.000833535, z: 5.77815e-05\n",
      "Velocity: x: -0.00192317, y: -0.0271403, z: 0.00129111\n",
      "Acceleration: x: -0.0191948, y: -0.470966, z: 0.0107302\n",
      "\n",
      "Frame #25\n",
      "Position: x: -0.000107518, y: -0.000974454, z: 6.63489e-05\n",
      "Velocity: x: -0.00200455, y: -0.0293824, z: 0.0016214\n",
      "Acceleration: x: -0.0183142, y: -0.46436, z: 0.0265727\n",
      "\n",
      "Frame #26\n",
      "Position: x: -0.000117646, y: -0.00112594, z: 7.82767e-05\n",
      "Velocity: x: -0.00207873, y: -0.0315698, z: 0.00210299\n",
      "Acceleration: x: -0.0175065, y: -0.458285, z: 0.0420915\n",
      "\n",
      "Frame #27\n",
      "Position: x: -0.000127294, y: -0.0012824, z: 9.90787e-05\n",
      "Velocity: x: -0.00203915, y: -0.0329425, z: 0.0034898\n",
      "Acceleration: x: -0.00817231, y: -0.391076, z: 0.124678\n",
      "\n",
      "Frame #28\n",
      "Position: x: -0.000137644, y: -0.00145234, z: 0.000117836\n",
      "Velocity: x: -0.00209162, y: -0.0349835, z: 0.00397269\n",
      "Acceleration: x: -0.00917966, y: -0.398518, z: 0.112103\n",
      "\n",
      "Frame #29\n",
      "Position: x: -0.000148313, y: -0.00163293, z: 0.000138251\n",
      "Velocity: x: -0.0021499, y: -0.0370678, z: 0.00437716\n",
      "Acceleration: x: -0.00992638, y: -0.404038, z: 0.102457\n",
      "\n",
      "Frame #30\n",
      "Position: x: -6.89305e-05, y: -0.000653777, z: 2.94624e-05\n",
      "Velocity: x: -0.00079441, y: -0.0237552, z: -0.000381444\n",
      "Acceleration: x: -0.00938612, y: -0.456046, z: -0.0285476\n",
      "\n",
      "Frame #31\n",
      "Position: x: -7.31466e-05, y: -0.000777344, z: 2.98056e-05\n",
      "Velocity: x: -0.000834618, y: -0.0259503, z: -0.000278064\n",
      "Acceleration: x: -0.0088748, y: -0.451994, z: -0.0168458\n",
      "\n",
      "Frame #32\n",
      "Position: x: -7.76936e-05, y: -0.000912636, z: 3.20189e-05\n",
      "Velocity: x: -0.000882753, y: -0.0281984, z: -6.99196e-05\n",
      "Acceleration: x: -0.00885915, y: -0.451497, z: -0.00561011\n",
      "\n",
      "Frame #33\n",
      "Position: x: -8.24336e-05, y: -0.00105901, z: 4.01867e-05\n",
      "Velocity: x: -0.000909212, y: -0.0303014, z: 0.000796622\n",
      "Acceleration: x: -0.00657088, y: -0.435419, z: 0.0528707\n",
      "\n",
      "Frame #34\n",
      "Position: x: -8.7328e-05, y: -0.00121652, z: 4.53348e-05\n",
      "Velocity: x: -0.000936297, y: -0.0324161, z: 0.000921117\n",
      "Acceleration: x: -0.00563493, y: -0.428704, z: 0.0387732\n",
      "\n",
      "Frame #35\n",
      "Position: x: -9.22371e-05, y: -0.0013841, z: 5.01576e-05\n",
      "Velocity: x: -0.000960758, y: -0.0345193, z: 0.000957354\n",
      "Acceleration: x: -0.00522945, y: -0.425815, z: 0.0281996\n",
      "\n",
      "Frame #36\n",
      "Position: x: -5.29494e-06, y: -0.000609975, z: 4.54373e-05\n",
      "Velocity: x: 0.00045695, y: -0.0227958, z: 0.000778932\n",
      "Acceleration: x: -0.00220189, y: -0.450801, z: -0.00102906\n",
      "\n",
      "Frame #37\n",
      "Position: x: -3.30373e-06, y: -0.000731224, z: 6.17972e-05\n",
      "Velocity: x: 0.000433481, y: -0.0251741, z: 0.00189678\n",
      "Acceleration: x: -0.00263398, y: -0.456802, z: 0.0598468\n",
      "\n",
      "Frame #38\n",
      "Position: x: -1.19559e-06, y: -0.00086262, z: 7.0962e-05\n",
      "Velocity: x: 0.000424591, y: -0.0274219, z: 0.00204897\n",
      "Acceleration: x: -0.00232321, y: -0.454645, z: 0.0514463\n",
      "\n",
      "Frame #39\n",
      "Position: x: 8.63651e-07, y: -0.00100535, z: 8.01539e-05\n",
      "Velocity: x: 0.000414888, y: -0.0296774, z: 0.00213791\n",
      "Acceleration: x: -0.00219146, y: -0.453761, z: 0.0439683\n",
      "\n",
      "Frame #40\n",
      "Position: x: 3.26836e-06, y: -0.0011567, z: 8.17984e-05\n",
      "Velocity: x: 0.000464758, y: -0.0315433, z: 0.00118444\n",
      "Acceleration: x: 0.00273177, y: -0.42156, z: -0.039441\n",
      "\n",
      "Frame #41\n",
      "Position: x: 5.5606e-06, y: -0.00131983, z: 8.75685e-05\n",
      "Velocity: x: 0.00047903, y: -0.0336386, z: 0.00116914\n",
      "Acceleration: x: 0.00287651, y: -0.420272, z: -0.0232699\n",
      "\n",
      "Frame #42\n",
      "Position: x: 7.94783e-06, y: -0.00149341, z: 9.40913e-05\n",
      "Velocity: x: 0.000493191, y: -0.0357362, z: 0.00122729\n",
      "Acceleration: x: 0.00290264, y: -0.419918, z: -0.0125366\n",
      "\n",
      "Frame #43\n",
      "Position: x: -1.45715e-05, y: -0.000603481, z: -1.49541e-05\n",
      "Velocity: x: -0.000124176, y: -0.021978, z: 0.000691802\n",
      "Acceleration: x: -0.00248483, y: -0.427304, z: 0.0395433\n",
      "\n",
      "Frame #44\n",
      "Position: x: -1.52278e-05, y: -0.000718621, z: -1.18435e-05\n",
      "Velocity: x: -0.000132969, y: -0.024088, z: 0.000789308\n",
      "Acceleration: x: -0.00225011, y: -0.425675, z: 0.0339569\n",
      "\n",
      "Frame #45\n",
      "Position: x: -1.58003e-05, y: -0.000843512, z: -7.88049e-06\n",
      "Velocity: x: -0.000131804, y: -0.0261274, z: 0.000922147\n",
      "Acceleration: x: -0.0016971, y: -0.421708, z: 0.0323459\n",
      "\n",
      "Frame #46\n",
      "Position: x: -1.4121e-05, y: -0.000974954, z: -4.965e-06\n",
      "Velocity: x: 0.000184593, y: -0.0276674, z: 0.000777848\n",
      "Acceleration: x: 0.0236991, y: -0.380207, z: 0.00759406\n",
      "\n",
      "Frame #47\n",
      "Position: x: -1.28917e-05, y: -0.0011182, z: -9.99904e-07\n",
      "Velocity: x: 0.00030008, y: -0.0295942, z: 0.000775221\n",
      "Acceleration: x: 0.0234034, y: -0.382384, z: 0.00389642\n",
      "\n",
      "Frame #48\n",
      "Position: x: -1.10893e-05, y: -0.00127118, z: 2.52571e-06\n",
      "Velocity: x: 0.000415356, y: -0.0315241, z: 0.000707005\n",
      "Acceleration: x: 0.0232748, y: -0.38339, z: -0.00158858\n",
      "\n",
      "Frame #49\n",
      "Position: x: -1.10294e-05, y: -0.00143824, z: 4.13834e-06\n",
      "Velocity: x: 0.000200144, y: -0.034082, z: 0.000445588\n",
      "Acceleration: x: -0.00303472, y: -0.43287, z: -0.0170297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "## License: Apache 2.0. See LICENSE file in root directory.\n",
    "## Copyright(c) 2019 Intel Corporation. All Rights Reserved.\n",
    "\n",
    "#####################################################\n",
    "##           librealsense T265 example             ##\n",
    "#####################################################\n",
    "\n",
    "# First import the library\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "# Declare RealSense pipeline, encapsulating the actual device and sensors\n",
    "pipe = rs.pipeline()\n",
    "\n",
    "# Build config object and request pose data\n",
    "cfg = rs.config()\n",
    "cfg.enable_stream(rs.stream.pose)\n",
    "\n",
    "# Start streaming with requested config\n",
    "pipe.start(cfg)\n",
    "\n",
    "try:\n",
    "    for _ in range(50):\n",
    "        # Wait for the next set of frames from the camera\n",
    "        frames = pipe.wait_for_frames()\n",
    "\n",
    "        # Fetch pose frame\n",
    "        pose = frames.get_pose_frame()\n",
    "        if pose:\n",
    "            # Print some of the pose data to the terminal\n",
    "            data = pose.get_pose_data()\n",
    "            print(\"Frame #{}\".format(pose.frame_number))\n",
    "            print(\"Position: {}\".format(data.translation))\n",
    "            print(\"Velocity: {}\".format(data.velocity))\n",
    "            print(\"Acceleration: {}\\n\".format(data.acceleration))\n",
    "\n",
    "finally:\n",
    "    pipe.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "## License: Apache 2.0. See LICENSE file in root directory.\n",
    "## Copyright(c) 2019 Intel Corporation. All Rights Reserved.\n",
    "\n",
    "#####################################################\n",
    "##           librealsense T265 rpy example         ##\n",
    "#####################################################\n",
    "\n",
    "# First import the library\n",
    "import pyrealsense2 as rs\n",
    "import math as m\n",
    "\n",
    "# Declare RealSense pipeline, encapsulating the actual device and sensors\n",
    "pipe = rs.pipeline()\n",
    "\n",
    "# Build config object and request pose data\n",
    "cfg = rs.config()\n",
    "cfg.enable_stream(rs.stream.pose)\n",
    "\n",
    "# Start streaming with requested config\n",
    "pipe.start(cfg)\n",
    "\n",
    "try:\n",
    "    while (True):\n",
    "        # Wait for the next set of frames from the camera\n",
    "        frames = pipe.wait_for_frames()\n",
    "\n",
    "        # Fetch pose frame\n",
    "        pose = frames.get_pose_frame()\n",
    "        if pose:\n",
    "            # Print some of the pose data to the terminal\n",
    "            data = pose.get_pose_data()\n",
    "\n",
    "            # Euler angles from pose quaternion\n",
    "            # See also https://github.com/IntelRealSense/librealsense/issues/5178#issuecomment-549795232\n",
    "            # and https://github.com/IntelRealSense/librealsense/issues/5178#issuecomment-550217609\n",
    "\n",
    "            w = data.rotation.w\n",
    "            x = -data.rotation.z\n",
    "            y = data.rotation.x\n",
    "            z = -data.rotation.y\n",
    "\n",
    "            pitch =  -m.asin(2.0 * (x*z - w*y)) * 180.0 / m.pi;\n",
    "            roll  =  m.atan2(2.0 * (w*x + y*z), w*w - x*x - y*y + z*z) * 180.0 / m.pi;\n",
    "            yaw   =  m.atan2(2.0 * (w*z + x*y), w*w + x*x - y*y - z*z) * 180.0 / m.pi;\n",
    "            \n",
    "            print(\"Frame #{}\".format(pose.frame_number))\n",
    "            print(\"RPY [deg]: Roll: {0:.7f}, Pitch: {1:.7f}, Yaw: {2:.7f}\".format(roll, pitch, yaw))\n",
    "\n",
    "\n",
    "finally:\n",
    "    pipe.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No device connected",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\飞控\\flyzhu\\数字识别\\t265.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E9%A3%9E%E6%8E%A7/flyzhu/%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/t265.ipynb#ch0000003?line=22'>23</a>\u001b[0m pipe \u001b[39m=\u001b[39m rs\u001b[39m.\u001b[39mpipeline()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E9%A3%9E%E6%8E%A7/flyzhu/%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/t265.ipynb#ch0000003?line=23'>24</a>\u001b[0m cfg \u001b[39m=\u001b[39m rs\u001b[39m.\u001b[39mconfig()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%E9%A3%9E%E6%8E%A7/flyzhu/%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/t265.ipynb#ch0000003?line=24'>25</a>\u001b[0m profile \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39;49mresolve(pipe)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E9%A3%9E%E6%8E%A7/flyzhu/%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/t265.ipynb#ch0000003?line=25'>26</a>\u001b[0m dev \u001b[39m=\u001b[39m profile\u001b[39m.\u001b[39mget_device()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E9%A3%9E%E6%8E%A7/flyzhu/%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/t265.ipynb#ch0000003?line=26'>27</a>\u001b[0m tm2 \u001b[39m=\u001b[39m dev\u001b[39m.\u001b[39mas_tm2()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No device connected"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "## License: Apache 2.0. See LICENSE file in root directory.\n",
    "## Copyright(c) 2019 Intel Corporation. All Rights Reserved.\n",
    "from __future__ import print_function\n",
    "\n",
    "\"\"\"\n",
    "This example shows how to fuse wheel odometry measurements (in the form of 3D translational velocity measurements) on the T265 tracking camera to use them together with the (internal) visual and intertial measurements.\n",
    "This functionality makes use of two API calls:\n",
    "1. Configuring the wheel odometry by providing a json calibration file (in the format of the accompanying calibration file)\n",
    "Please refer to the description of the calibration file format here: https://github.com/IntelRealSense/librealsense/blob/master/doc/t265.md#wheel-odometry-calibration-file-format.\n",
    "2. Sending wheel odometry measurements (for every measurement) to the camera\n",
    "Expected output:\n",
    "For a static camera, the pose output is expected to move in the direction of the (artificial) wheel odometry measurements (taking into account the extrinsics in the calibration file).\n",
    "The measurements are given a high weight/confidence, i.e. low measurement noise covariance, in the calibration file to make the effect visible.\n",
    "If the camera is partially occluded the effect will be even more visible (also for a smaller wheel odometry confidence / higher measurement noise covariance) because of the lack of visual feedback. Please note that if the camera is *fully* occluded the pose estimation will switch to 3DOF, estimate only orientation, and prevent any changes in the position.\n",
    "\"\"\"\n",
    "\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "# load wheel odometry config before pipe.start(...)\n",
    "# get profile/device/ wheel odometry sensor by profile = cfg.resolve(pipe)\n",
    "pipe = rs.pipeline()\n",
    "cfg = rs.config()\n",
    "profile = cfg.resolve(pipe)\n",
    "dev = profile.get_device()\n",
    "tm2 = dev.as_tm2()\n",
    "\n",
    "if(tm2):\n",
    "    # tm2.first_wheel_odometer()?\n",
    "    pose_sensor = tm2.first_pose_sensor()\n",
    "    wheel_odometer = pose_sensor.as_wheel_odometer()\n",
    "\n",
    "    # calibration to list of uint8\n",
    "    f = open(\"calibration_odometry.json\")\n",
    "    chars = []\n",
    "    for line in f:\n",
    "       for c in line:\n",
    "           chars.append(ord(c))  # char to uint8\n",
    "\n",
    "    # load/configure wheel odometer\n",
    "    wheel_odometer.load_wheel_odometery_config(chars)\n",
    "\n",
    "\n",
    "    pipe.start()\n",
    "    try:\n",
    "        for _ in range(100):\n",
    "            frames = pipe.wait_for_frames()\n",
    "            pose = frames.get_pose_frame()\n",
    "            if pose:\n",
    "                data = pose.get_pose_data()\n",
    "                print(\"Frame #{}\".format(pose.frame_number))\n",
    "                print(\"Position: {}\".format(data.translation))\n",
    "\n",
    "                # provide wheel odometry as vecocity measurement\n",
    "                wo_sensor_id = 0  # indexed from 0, match to order in calibration file\n",
    "                frame_num = 0  # not used\n",
    "                v = rs.vector()\n",
    "                v.x = 0.1  # m/s\n",
    "                wheel_odometer.send_wheel_odometry(wo_sensor_id, frame_num, v)\n",
    "    finally:\n",
    "        pipe.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95d50a5c078e45808b0310509146bdfa25d1e01a1940ba5c1a212b3c73de28e0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
